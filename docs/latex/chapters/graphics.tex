%----------------------------------------------------------------------------
\chapter{Presentation layer}

The frontend is compiled against Java 21, and is built using multiple scenes created using libGDX. The program is completely self-contained, with only a few open-source asset files and fonts required in addition to the declared dependencies -- these are included in the repository.

\section{The rendering pipeline}

The OpenGL pipeline consists of the following steps: vertex processing, vertex post-processing, fragment generation, processing using a fragment shader, per-sample processing\cite{OpenglPipeline}. LibGDX provides calls to the low-level equivalents of each of these steps. As with all other graphics programming problems, the primary objective is to reduce the data sent between the CPU and the GPU, and to minimise GPU context switching.

The GPU may receive information for rendering a model or mesh in two ways. The CPU could pre-calculate everything: model nodes, geometry, normal vectors, materials, colouring, and lighting. For the average programmer, this method is far more desirable. All operations occur in-memory, using a familiar programming language. Crashes are far easier to investigate, as the local stack trace can reveal most issues. However, the processor is generally slower at three-dimensional calculations, as it's not built specifically for matrix mathematics. The Peripheral Component Interconnect Express (PCI-e) bus may also become clogged with these models, as it is already often the bottleneck. If the bus can't deliver frame data from the graphics card to the processor, the image may stall or stutter.

The graphics processor can also be put to work for calculating lighting, shading and even orientation. All steps aside from the first one can be easily done using a vertex shader and a fragment shader program. These are written in the OpenGL Shading Language (GLSL)\footnote{\url{https://www.khronos.org/opengl/wiki/Core_Language_(GLSL)}}, which is syntactically based on the C programming language, with subtle changes to allow for a more functional style of data transformation. Incoming vertices are most often moved into place, scaled, and have colours applied to them to create the final look. The PCI-e bus and CPU can focus on other, more specific work, as only the initial vertices need to be sent. Shader optimisation is important in its own right, as it can allow for huge efficiency gains for the entirety of any visual application.

OpenGL is fastest when the type and structure of rendered data does not change. Any settings change or new model may be considered a context change, and can cause frame drops. Batching may be used to combine the geometry of multiple or all models, allowing the GPU to render each frame with absolutely no switches. Too many nodes contained in a single model may cause issues though, as a change to any part may require a complete recreation.

Caching is similar in principle, but more flexible. Instead of rendering the original models, a cache may be created that accounts for layering and invisible faces, dropping anything that isn't needed for a complete image. A cache also requires recompilation when parts of it change, but that's easier to handle.

The processing load may also be reduced by simplifying the scene as much as possible. Objects outside of the current field of view don't need to be rendered, this is called frustum culling. The camera's view frustum is the area between the set near and far planes (given distances from our viewpoint), with a rectangular cone-shape projected out from the camera. Far-away objects can also be rendered at a lower complexity, dropping nodes in order to create lower level of detail (LOD) models. This trades more setup work for better runtime performance.

\section{Low level graphical challenges}

By using near-machine graphical interfaces, we lose certain simplified operations, for instance the ability to draw complex three-dimensional shapes directly from code. Most graphics cards have certain predispositions about the data they process, and in low-level cases, our program can't guarantee that those rules will be followed. LibGDX provides strong foundations in place of ready-made solutions, which the developer can use to implement or in some cases, disregard the GPU's written or unwritten requests. These include the following examples.

\subsection{Triangulation, ear-clipping algorithm}

Triangles are the most commonly used basic two-dimensional shape that is supported without too many caveats on all platforms. Therefore, it's advisable to reduce any displayed models to a group of them.

One geometrically intuitive division operation is the ear-clipping algorithm. An ear is defined as a triangle made up of three successive vertices of a polygon, where the middle vertex is convex -- in that point, the shape's inner angle must not exceed 180 degrees ($\pi$ radians).\cite{TriangulationByEarClipping} Areas defined by such triples are guaranteed to be subtractable from the polygon's area, since by definition, they can not contain any holes, removing the need to compensate for empty space. The action can then be repeated on the remaining area, each successive step will result in a new triangular piece; when only three vertices remain, we have achieved our goal of subdivision.

The goal of this algorithm is to create a list of vertex indices that should be connected into triangles, in order to cover the original polygon fully and efficiently. As a useful side effect, this makes calculating the mathematical area trivial as well.

In the case of libGDX, the EarClippingTriangulator class takes care of this common use case. Following OpenGL idioms, the relevant function requests floating point values (vectors decomposed into their axes), and it results in a list of whole number indices -- every group of three defines a triangle to be created.

\begin{lstlisting}[caption=Example usage of the EarClippingTriangulator class]
    val floats = baseNodes.flatMap { listOf(it.x, it.z) }.toFloatArray()
    val triangles: ShortArray
    val triangulator = EarClippingTriangulator()
    triangles = triangulator.computeTriangles(floats)
\end{lstlisting}

\subsection{Winding order}

Many graphical effects - including lighting - require knowledge of the affected faces' normal vectors. In the mathematical sense, this means a direction which is perpendicular to the whole surface, when applied to any contained point of it. %TODO meh 
When observing the side from this particular direction (the vector between the camera and a given point of the face can be represented as a multiple of the normal), the apparent size is at its largest -- in layman's terms, we're seeing the front without any rotations around axes that don't match the normal.

The visibility of a polygon can technically also be treated as an effect. To reduce the number of draw calls, it's advisable to only handle faces that have their intended front side visible from the camera's perspective. By default, OpenGL utilises "back-face culling": the spectator can only see shapes where the order of vertices is counter-clockwise (when plotted in relation to the vector connecting the viewpoint and the weighted center of the polygon). This behaviour is modifiable as follows.

\begin{lstlisting}[caption=Example for changing OpenGL's culling properties through libGDX]
    Gdx.gl.glEnable(GL40.GL_CULL_FACE) //enabling
    Gdx.gl.glCullFace(GL40.GL_BACK) //setting filtered side (rear)
    Gdx.gl.glFront(GL40.CW) //changing the intended winding order to clockwise
\end{lstlisting}

\subsection{Using the main thread}

In contrast to the project's simulation module, the user interface is difficult to parallelise. OpenGL functions are usually not thread-safe, requesting and modifying a single graphical context from multiple areas of code is dangerous and will often lead to crashes.

The graphics library abstracts communication with the operating system and GPU as much as possible. In return, it's the developer's responsibility to reduce GPU calls to the absolutely necessary amount, as previously mentioned. Aside from frame construction, the program only uses the main graphical thread for creating unique models for each building, as this can't reasonably be done on the CPU.\@I decided against a complex thread-scheduling solution, as the GPU only receives a heavy workload when creating a new save, or loading an OSM buffer file. When programming, I only had to build on available tools; I used a simple lambda function to obtain the main thread context when needed.

\begin{lstlisting}[caption=Helper function for getting the draw thread]
private suspend fun <T> runOnRenderThread(block: () -> T): T {
        return suspendCoroutine { continuation ->
            Gdx.app.postRunnable {
                continuation.resume(block())
            }
        }
    }
\end{lstlisting}

\section{Implementation}
The program's visual part can be divided into modules. Aside from the interactive, three-dimensional OpenGL scene showcasing the buildings, there exist multiple independent components, aiding user communication.

\section{The main scene}
Buildings are presented in a simplified manner, ignoring any vertical terrain changes. Stylistically, it was inspired by contemporary city building video games (such as SimCity and Cities: Skylines), as well Google Earth.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=150mm, keepaspectratio]{images/main_graphics_view.png}
    \caption{The main page of the app, with a "false 3D" view}
\end{figure}

\subsection{Custom camera control}

For libGDX scenes, it is possible to inject a single input processing adapter object, or a combination of multiple objects known as a multiplexer. In the recommended starter 3D program\cite{basic3DlibGDX}, the library's creators added the CameraInputController class; this event handler provides mouse-controlled rotation and zooming for its containing scene.

I created my own camera and menu functions based on the public InputAdapter interface. The viewing angle is locked, the user can inspect the image made from backend-sourced data from a fixed top-down perspective. Moving exploits the need for updating the camera every frame; by pressing a navigational button, a certain target amount is added to the handler responsible for the given axis. This number reduces by a certain percentage each frame, zeroing out the value once it gets below a certain threshold.

This results in smooth speed handling, which avoids robotic, sudden direction changes. Since multiple commands may be executed at once, every function that changes the camera's position must be under a mutex (mutually exclusive) lock. Without this, multiple camera location changes cause the currently rendered frame to flicker.

%TODO: more info about the scenes